---
title: "`marginaleffects` for Python"
format: gfm
---

The `marginaleffects` package allows `Python` users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons, and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, "marginal means", which is a special case of averaged predictions. `marginaleffects` can also average (or "marginalize") unit-level (or "conditional") estimates of all those quantities, and conduct hypothesis tests on them.

## WARNING

This is an *alpha* version of the package, released to gather feedback, feature requests, and bug reports from potential users. This version includes known bugs. There are also known discrepancies between the numerical results produced in Python and R. Please report any issues you encounter here: https://github.com/vincentarelbundock/pymarginaleffects/issues


## Supported models

There is a good chance that this package will work with (nearly) all the models supported by [the `statsmodels` formula API,](https://www.statsmodels.org/stable/api.html#statsmodels-formula-api) ex: `ols`, `probit`, `logit`, `mnlogit`, `quantreg`, `poisson`, `negativebinomial`, `mixedlm`, `rlm`, etc. However, the package has only been tested with a subset of those, and some weirdness remains. Again: this is *alpha* software; it should not be used in critical applications yet.


## Installation

Install the latest PyPi release:

```{python}
#| eval: false
pip install marginaleffects
```

## Estimands: Predictions, Comparisons, and Slopes


## Definitions

[_Predictions_:](predictions.html)

> The outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. `predictions()`, `avg_predictions()`, `plot_predictions()`.

[_Comparisons_:](comparisons.html)

> Compare the predictions made by a model for different regressor values (e.g., college graduates vs. others): contrasts, differences, risk ratios, odds, etc. `comparisons()`, `avg_comparisons()`, `plot_comparisons()`.

[_Slopes_:](slopes.html) 

> Partial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. `slopes()`, `avg_slopes()`, `plot_slopes()`.

[Hypothesis and Equivalence Tests:](hypothesis.html)

> Hypothesis and equivalence tests can be conducted on linear or non-linear functions of model coefficients, or on any of the quantities computed by the `marginaleffects` packages (predictions, slopes, comparisons, marginal means, etc.). Uncertainy estimates can be obtained via the delta method (with or without robust standard errors), bootstrap, or simulation.

Predictions, comparisons, and slopes are fundamentally unit-level (or "conditional") quantities. Except in the simplest linear case, estimates will typically vary based on the values of all the regressors in a model. Each of the observations in a dataset is thus associated with its own prediction, comparison, and slope estimates. Below, we will see that it can be useful to marginalize (or "average over") unit-level estimates to report an "average prediction", "average comparison", or "average slope".

We now apply `marginaleffects` functions to compute each of the estimands described above. First, we fit a linear regression model with multiplicative interactions:

#### Predictions

```{python}
import numpy as np
import polars as pl
from marginaleffects import *
import statsmodels.formula.api as smf
mtcars = pl.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv")
mod = smf.ols("mpg ~ hp * wt * am", data = mtcars).fit()

mod.summary()
```

```{python}
#| include: false
pl.Config(
    tbl_formatting="ASCII_MARKDOWN",
    tbl_hide_column_data_types=True,
    tbl_hide_dataframe_shape=True,
)
pl.Config.set_tbl_cols(8)
```

Then, we call the `predictions()` function. As noted above, predictions are unit-level estimates, so there is one specific prediction per observation. By default, the `predictions()` function makes one prediction per observation in the dataset that was used to fit the original model. Since `mtcars` has 32 rows, the `predictions()` outcome also has 32 rows:

```{python}
pre = predictions(mod)

pre.shape

pre.head()
```

#### Comparisons: Differences, Ratios, Log-Odds, Lift, etc.

Now, we use the `comparisons()` function to compute the difference in predicted outcome when each of the predictors is incremented by 1 unit (one predictor at a time, holding all others constant). Once again, comparisons are unit-level quantities. And since there are 3 predictors in the model and our data has 32 rows, we obtain 96 comparisons:

```{python}
cmp = comparisons(mod)

cmp.shape

cmp.head()
```

The `comparisons()` function allows customized queries. For example, what happens to the predicted outcome when the `hp` variable increases from 100 to 120?

```{python}
cmp = comparisons(mod, variables = {"hp": [120, 100]})
cmp
```

What happens to the predicted outcome when the `wt` variable increases by 1 standard deviation about its mean?

```{python}
cmp = comparisons(mod, variables = {"hp": "sd"})
cmp
```

The `comparisons()` function also allows users to specify arbitrary functions of predictions, with the `comparison` argument. For example, what is the average ratio between predicted Miles per Gallon after an increase of 50 units in Horsepower?

```{python}
cmp = comparisons(
  mod,
  variables = {"hp": 50},
  comparison = "ratioavg")
cmp
```

#### Slopes: Derivatives and elasticities

Consider a logistic regression model with a single predictor:

```{python}
url = "https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv"
mtcars = pl.read_csv(url)
mod = smf.logit("am ~ mpg", data = mtcars).fit()
```

We can estimate the slope of the prediction function with respect to the `mpg` variable at any point in the data space. For example, what is the slope of the prediction function at `mpg = 24`?

```{python}
mfx = slopes(mod, newdata = datagrid(mpg = 24, newdata = mtcars))
mfx
```

This is equivalent to the result we obtain by taking the analytical derivative using the chain rule:

```{python}
from scipy.stats import logistic
beta_0 = mod.params.iloc[0]
beta_1 = mod.params.iloc[1]
print(beta_1 * logistic.pdf(beta_0 + beta_1 * 24))
```

This computes a "marginal effect (or slope) at the mean" or "at the median", that is, when all covariates are held at their mean or median values:

```{python}
mfx = slopes(mod, newdata = "mean")
mfx
```
```{python}
mfx = slopes(mod, newdata = "median")
mfx
```

We can also compute an "average slope" or "average marginaleffects"

```{python}
mfx = avg_slopes(mod)
mfx
```

Which again is equivalent to the analytical result:

```{python}
np.mean(beta_1 * logistic.pdf(beta_0 + beta_1 * mtcars["mpg"]))
```

## Grid

Predictions, comparisons, and slopes are typically "conditional" quantities which depend on the values of all the predictors in the model. By default, `marginaleffects` functions estimate quantities of interest for the empirical distribution of the data (i.e., for each row of the original dataset). However, users can specify the exact values of the predictors they want to investigate by using the `newdata` argument.

`newdata` accepts data frames like this:

```{python}
pre = predictions(mod, newdata = mtcars.tail(2))
pre
```

The [`datagrid` function gives us a powerful way to define a grid of predictors.](https://vincentarelbundock.github.io/marginaleffects/reference/datagrid.html) All the variables not mentioned explicitly in `datagrid()` are fixed to their mean or mode:

```{python}
pre = predictions(
  mod,
  newdata = datagrid(
    newdata = mtcars,
    am = [0, 1],
    wt = [mtcars["wt"].max(), mtcars["wt"].min()]))

pre
```

## Averaging

Since predictions, comparisons, and slopes are conditional quantities, they can be a bit unwieldy. Often, it can be useful to report a one-number summary instead of one estimate per observation. Instead of presenting "conditional" estimates, some methodologists recommend reporting "marginal" estimates, that is, an average of unit-level estimates. 

(This use of the word "marginal" as "averaging" should not be confused with the term "marginal effect" which, in the econometrics tradition, corresponds to a partial derivative, or the effect of a "small/marginal" change.)

To marginalize (average over) our unit-level estimates, we can use the `by` argument or the one of the convenience functions: `avg_predictions()`, `avg_comparisons()`, or `avg_slopes()`. For example, both of these commands give us the same result: the average predicted outcome in the `mtcars` dataset:

```{python}
pre = avg_predictions(mod)
pre
```

This is equivalent to manual computation by:

```{python}
np.mean(mod.predict())
```

The main `marginaleffects` functions all include a `by` argument, which allows us to marginalize within sub-groups of the data. For example,

```{python}
cmp = avg_comparisons(mod, by = "am")
cmp
```

Marginal Means are a special case of predictions, which are marginalized (or averaged) across a balanced grid of categorical predictors. To illustrate, we estimate a new model with categorical predictors:

```{python}
dat = mtcars \
  .with_columns(
    pl.col("am").cast(pl.Boolean),
    pl.col("cyl").cast(pl.Utf8)
  )
mod_cat = smf.ols("mpg ~ am + cyl + hp", data = dat).fit()
```

We can compute marginal means manually using the functions already described:

```{python}
#| eval: false
pre = avg_predictions(
  mod_cat,
  newdata = datagrid(
    newdata = dat,
    cyl = dat["cyl"].unique(),
    am = dat["am"].unique()),
  by = "am")

pre
```

```{python}
cmp = avg_comparisons(mod_cat)
cmp
```


## Hypothesis and equivalence tests

The `hypotheses()` function and the `hypothesis` argument can be used to conduct linear and non-linear hypothesis tests on model coefficients, or on any of the quantities computed by the functions introduced above.

Consider this model:

```{python}
mod = smf.ols("mpg ~ qsec * drat", data = mtcars).fit()
mod.params
```

Can we reject the null hypothesis that the `drat` coefficient is 2 times the size of the `qsec` coefficient?

```{python}
hyp = hypotheses(mod, "b3 = 2 * b2")
hyp
```

The main functions in `marginaleffects` all have a `hypothesis` argument, which means that we can do complex model testing. For example, consider two slope estimates:

```{python}
range = lambda x: [x.max(), x.min()]
cmp = comparisons(
  mod,
  variables = "drat",
  newdata = datagrid(newdata = mtcars, qsec = range(mtcars["qsec"])))
cmp)
```

Are these two contrasts significantly different from one another? To test this, we can use the `hypothesis` argument:

```{python}
cmp = comparisons(
  mod,
  hypothesis = "b1 = b2",
  variables = "drat",
  newdata = datagrid(newdata = mtcars, qsec = range(mtcars["qsec"])))
cmp)
```